工具类

[CVPR 2023 论文和开源项目合集(Papers with Code)](https://github.com/amusi/CVPR2023-Papers-with-Code)

[记录每天整理的计算机视觉/深度学习/机器学习相关方向的论文](https://github.com/amusi/daily-paper-computer-vision)

[Awesome-Data-Fusion-for-Remote-Sensing](https://github.com/px39n/Awesome-Data-Fusion-for-Remote-Sensing)

[Hyperspectral-Image-Super-Resolution-Benchmark](https://github.com/junjun-jiang/Hyperspectral-Image-Super-Resolution-Benchmark)

[Awesome-Data-Fusion-for-Remote-Sensing](https://github.com/px39n/Awesome-Data-Fusion-for-Remote-Sensing)





学习类



:ballot_box_with_check: [（附文献和代码）多源遥感影像时空融合技术介绍](https://zhuanlan.zhihu.com/p/601897889)

[图像融合常用数据集整理](https://zhuanlan.zhihu.com/p/508051065)

[[遥感]遥感**变化检测**文献/数据集资源列表推荐](https://zhuanlan.zhihu.com/p/528959742)

[笔记 | 基于深度学习的空谱遥感图像融合综述](https://zhuanlan.zhihu.com/p/569415087?utm_id=0)

[遥感分辨率及图像知识大全](https://zhuanlan.zhihu.com/p/437700025)

[条件生成对抗网络——cGAN原理与代码](https://zhuanlan.zhihu.com/p/629503280?utm_id=0)

[AIGC之图像生成内容介绍](https://zhuanlan.zhihu.com/p/629914637)

[VAE-变分自编码器系列](https://cloud.tencent.com/developer/article/2328570)



[Landsat 和 MODIS 数据介绍（针对CIA,LGC数据集）](https://zhuanlan.zhihu.com/p/415449467)





## 时空融合的目的

目前单一卫星传感器获取的影像数据无法同时兼顾高空间分辨率和高时 间分辨率。因此，国内外学者提出了许多遥感图像时空融合方法，来生成同时具 有高空间分辨率与高时间分辨率的遥感图像

其实可以看作是带更多约束条件的分辨率，其目的是为了获取连续的、高空间分辨率的影像，解决光学遥感影像在**时间分辨率和空间分辨率的矛盾**：现有的大多数卫星获取的影像在空间分辨率能做到较高，那么它的回访周期就很漫长。

- **通俗理解**

卫星带着相机对准地表拍摄，并绕着地球旋转，那如果卫星拍摄区域小，那拍出来的照片的就更加清晰（空间分辨率高），但是要拍完整个地球要花更多时间（时间分辨率低），如果卫星拍的面积更大，拍完整个地球一遍要花更少时间，但是照片更加模糊。



## 解决方法

那么如何才能获取高时空分辨率的影像呢？为什么要用时空融合技术来实现呢？

实际上，现有的办法主要有三种：

（1）多个搭载相同传感器的卫星组网，例如sentinel-2A和sentinel-2B双星，回访周期缩短一半；

（2）提高传感器性能和卫星的观测技术；

（3）将高时间分辨率影像和高空间分辨率影像进行融合来获取高时空分辨率影像。



## 基础原理

时空融合的基础原理如下图所示，其本质是结合两类影像的时空优势。

![img](image/资料收集/v2-60ca33f0bdcdfbd3fa76fb72ea5fc08d_720w.webp)

T1时相为我们能获取到两类影像的时相，T2时相是只有高时间分辨率影像的时相

上图只考虑了一对可用影像的情况，实际上很多算法会使用多对历史影像来提高融合的可靠性，如下图：

![img](image/资料收集/v2-8150de4ce4e2b14376d90c0015b6d602_720w.webp)

图片来自文献[https://doi.org/10.3390/rs10040527]



## STFDCNN

STFDCNN算法，该算法中引入了卷积神经网络来建模空间分辨率不一致的问题，来生成具有高时空的高分辨率图像







## GAN的提出和应用

GAN首先由Goodfellow等人提出[25]，然后，基于GAN的应用出现了爆炸式增长，例如图像生成[26]，超分辨率[27]，去噪[28]，修复[29]和图像到图像的翻译[30]，显示了其强大的建模能力。

[GAN详解](https://zhuanlan.zhihu.com/p/408766083)





## DCSTFN





## 略读论文

<span style='color:red'>红色字体</span>：表示疑问的地方

<span style="background-color: yellow;">黄色标记</span>：表示使用的方法

<span style=color:blue>蓝色字体</span>：表示解决的问题



###### Image De-raining Using a Conditional Generative Adversarial Network

摘要-恶劣的天气条件，如雨雪，对在这种条件下拍摄的图像的视觉质量产生不利影响，从而使它们无法进一步使用和共享。此外，这种退化的图像极大地影响了视觉系统的性能。因此，<span style=color:blue>解决单幅图像去雨问题</span>是非常重要的。然而，这个问题<span style='color:red'>固有的不适定性</span>带来了几个挑战。<span style="background-color: yellow;">我们试图利用最近引入的条件生成对抗网络(CGAN)的强大生成建模能力，**通过施加额外的约束**，即去雨图像必须与其对应的地面真实干净图像不可区分。来自GAN的对抗性损失提供了额外的规律性，并有助于实现更好的结果。除了提出一种去雨图像的新方法外，我们还**在生成器-鉴别器对中引入了一种新的精化损失函数和结构新颖性，以获得更好的结果**。Lost函数旨在减少Gans引入的伪影，并确保更好的视觉质量。生成器子网络是使用最近引入的密集连接网络来构建的，而鉴别器被设计为利用全局和局部信息来判断图像的真伪。</span>在此基础上，我们提出了一种新的单幅图像去雨方法--图像去雨条件生成对抗性网络(ID-CGAN)，该方法在目标函数中综合考虑了量化性能、视觉性能和判别性能。在合成图像和真实图像上的实验表明，该方法在量化和视觉性能方面都优于目前最先进的单幅图像去雨方法。此外，在FasterRCNN的目标检测数据集上的实验结果也证明了该方法在提高对降雨退化图像的检测性能方面的有效性。

[粗略讲解](https://blog.csdn.net/mmdbhs/article/details/122170935)





###### StfNet: A Two-Stream Convolutional Neural Network for Spatiotemporal Image Fusion

时空图像融合被认为是提供高空间分辨率和频繁覆盖地球观测的一种很有前途的方法，近年来，基于学习的解决方案受到了广泛的关注。然而，这些算法将时空融合处理为单幅图像的超分辨率问题，<span style=color:blue>在实际应用中，由于存在较大的升阶因子，使得粗图像的空间信息损失较大</span>。<span style="background-color: yellow;">为了解决这一问题，本文利用精细图像序列中的时间信息，使用一种称为StfNet的双流卷积神经网络来解决时空融合问题。这篇论文的新颖性是双重的。<br>(1)首先，考虑到图像序列之间的时间相关性，结合相邻日期获取的精细图像对预测日期的粗略图像进行超分辨。这样，我们的网络不仅通过粗细图像对之间的结构相似性来预测精细图像，而且还利用可用的相邻精细图像中丰富的纹理信息来预测精细图像。<br>(2)其次，**我们考虑了时间序列图像之间的时间关系，并制定了时间约束，而不是独立地估计每一幅输出的精细图像。这种时间约束旨在保证融合结果的唯一性，并鼓励在学习中进行时间一致的预测，从而产生更现实的最终结果**。</span>我们使用两个陆地卫星-中分辨率成像光谱仪(MODIS)采集的实际数据集来评估STfNet的性能，视觉和定量评估都表明我们的算法达到了最先进的性能。



1. 使用时序信息

2. 时空图像融合与经典的自然图像超分辨率融合区别

   首先,在时空融合中,放大倍数(通常从8到16)远远大于在超分辨率(通常从2到4)。在这种情况下,粗图像的纹理细节已经严重模糊和扭曲，并且能用于细图像预测的先验性结构信息很少。此外，我们知道，遥感图像与自然图像相比，包含更复杂的异质区域，纹理细节丰富，这进一步增加了精细图像预测的难度。因此，仅从空间结构相似度学习映射函数是一个很不适定的逆问题，之前学习的模型中精细图像和粗图像之间的关系可能会无效。因此，仅从对应的粗图像中无法准确预测出细图像，

3. 







###### Spatio-temporal fusion for remote sensing data: an overview and new benchmark 遥感时空融合：综述和新基准

摘要时空融合(STF)的目的是融合(时间密集的)粗分辨率图像和(时间稀疏的)精细分辨率图像，以生成具有足够的时间和空间分辨率的图像序列。在过去的十年里，STF引起了人们的极大关注，并发展了许多STF方法。然而，到目前为止，STF领域仍然缺乏基准数据集，这是一个迫切需要解决的问题，以促进该领域的发展。<span style=color:blue>在这篇综述中，我们(在文献中第一次)提供了一个健壮的基准STF数据集，它包括三个重要特征：(1)区域多样性，(2)长时间跨度，(3)具有挑战性的场景。我们还提供了具有高度代表性的STF技术的调查，以及它们的性能与我们新提出的基准数据集的详细定量和定性比较。拟议的数据集是公开的，并可在网上获得</span>。时空融合；多时相遥感数据；基准数据；实验验证

[【文献阅读】](https://zhuanlan.zhihu.com/p/471885193)



###### A ConvNet for the 2020s (CVPR 2022)

**摘要**

视觉识别的“兴旺的20年代”始于Vision Transformers (ViTs)的首次提出，它迅速取代了ConvNets，成为最先进的图像分类模型。另一方面，普通的ViT在应用于目标检测和语义分割等一般计算机视觉任务时面临很多困难。正是分层 Transformers（例如Swin Transformers）重新引入了几个ConvNet先验，使得 Transformers作为通用视觉主干实际上可行的，并在各类视觉任务上展现出卓越的性能。然而，这种混合方法的有效性在很大程度上仍归功于Transformers的内在优势，而不是卷积固有的归纳偏置。在这项工作中，我们重新检查了设计空间并测试了纯ConvNet所能达到的极限。我们逐渐将标准ResNet将视觉Transformers的设计方向进行“现代化”，并在此过程中发现了造成性能差异的几个关键组件。探索的结果是产生一系列纯ConvNet模型，称为ConvNeXt。ConvNeXts完全由标准ConvNet模块构建，在准确性和可扩展性方面与Transformers可匹敌，取得87.8% ImageNet top-1精度，在COCO检测和ADE20K分割方面优于Swin Transformers，同时保持标准ConvNet的简洁和效率。

[【论文简述及翻译】A ConvNet for the 2020s(CVPR 2022)](https://blog.csdn.net/qq_43307074/article/details/127247752)
