工具类

[CVPR 2023 论文和开源项目合集(Papers with Code)](https://github.com/amusi/CVPR2023-Papers-with-Code)

[记录每天整理的计算机视觉/深度学习/机器学习相关方向的论文](https://github.com/amusi/daily-paper-computer-vision)

[Awesome-Data-Fusion-for-Remote-Sensing](https://github.com/px39n/Awesome-Data-Fusion-for-Remote-Sensing)

[Hyperspectral-Image-Super-Resolution-Benchmark](https://github.com/junjun-jiang/Hyperspectral-Image-Super-Resolution-Benchmark)

[Awesome-Data-Fusion-for-Remote-Sensing](https://github.com/px39n/Awesome-Data-Fusion-for-Remote-Sensing)

[GPT 学术优化 (GPT Academic)](https://github.com/jio-H/gpt_academic)



学习类



:ballot_box_with_check: [（附文献和代码）多源遥感影像时空融合技术介绍](https://zhuanlan.zhihu.com/p/601897889)

[图像融合常用数据集整理](https://zhuanlan.zhihu.com/p/508051065)

[[遥感]遥感**变化检测**文献/数据集资源列表推荐](https://zhuanlan.zhihu.com/p/528959742)

[笔记 | 基于深度学习的空谱遥感图像融合综述](https://zhuanlan.zhihu.com/p/569415087?utm_id=0)

[遥感分辨率及图像知识大全](https://zhuanlan.zhihu.com/p/437700025)

[条件生成对抗网络——cGAN原理与代码](https://zhuanlan.zhihu.com/p/629503280?utm_id=0)

[AIGC之图像生成内容介绍](https://zhuanlan.zhihu.com/p/629914637)

[VAE-变分自编码器系列](https://cloud.tencent.com/developer/article/2328570)



[Landsat 和 MODIS 数据介绍（针对CIA,LGC数据集）](https://zhuanlan.zhihu.com/p/415449467)





## 时空融合的目的

目前单一卫星传感器获取的影像数据无法同时兼顾高空间分辨率和高时 间分辨率。因此，国内外学者提出了许多遥感图像时空融合方法，来生成同时具 有高空间分辨率与高时间分辨率的遥感图像

其实可以看作是带更多约束条件的分辨率，其目的是为了获取连续的、高空间分辨率的影像，解决光学遥感影像在**时间分辨率和空间分辨率的矛盾**：现有的大多数卫星获取的影像在空间分辨率能做到较高，那么它的回访周期就很漫长。

- **通俗理解**

卫星带着相机对准地表拍摄，并绕着地球旋转，那如果卫星拍摄区域小，那拍出来的照片的就更加清晰（空间分辨率高），但是要拍完整个地球要花更多时间（时间分辨率低），如果卫星拍的面积更大，拍完整个地球一遍要花更少时间，但是照片更加模糊。



## 解决方法

那么如何才能获取高时空分辨率的影像呢？为什么要用时空融合技术来实现呢？

实际上，现有的办法主要有三种：

（1）多个搭载相同传感器的卫星组网，例如sentinel-2A和sentinel-2B双星，回访周期缩短一半；

（2）提高传感器性能和卫星的观测技术；

（3）将高时间分辨率影像和高空间分辨率影像进行融合来获取高时空分辨率影像。



## 基础原理

时空融合的基础原理如下图所示，其本质是结合两类影像的时空优势。

![img](image/资料收集/v2-60ca33f0bdcdfbd3fa76fb72ea5fc08d_720w.webp)

T1时相为我们能获取到两类影像的时相，T2时相是只有高时间分辨率影像的时相

上图只考虑了一对可用影像的情况，实际上很多算法会使用多对历史影像来提高融合的可靠性，如下图：

![img](image/资料收集/v2-8150de4ce4e2b14376d90c0015b6d602_720w.webp)

图片来自文献[https://doi.org/10.3390/rs10040527]



## STFDCNN

STFDCNN算法，该算法中引入了卷积神经网络来建模空间分辨率不一致的问题，来生成具有高时空的高分辨率图像







## GAN的提出和应用

GAN首先由Goodfellow等人提出[25]，然后，基于GAN的应用出现了爆炸式增长，例如图像生成[26]，超分辨率[27]，去噪[28]，修复[29]和图像到图像的翻译[30]，显示了其强大的建模能力。

[GAN详解](https://zhuanlan.zhihu.com/p/408766083)





## DCSTFN





## 略读论文

<span style='color:red'>红色字体</span>：表示疑问的地方

<span style="background-color: yellow;">黄色标记</span>：表示使用的方法

<span style=color:blue>蓝色字体</span>：表示解决的问题



###### Image De-raining Using a Conditional Generative Adversarial Network

摘要-恶劣的天气条件，如雨雪，对在这种条件下拍摄的图像的视觉质量产生不利影响，从而使它们无法进一步使用和共享。此外，这种退化的图像极大地影响了视觉系统的性能。因此，<span style=color:blue>解决单幅图像去雨问题</span>是非常重要的。然而，这个问题<span style='color:red'>固有的不适定性</span>带来了几个挑战。<span style="background-color: yellow;">我们试图利用最近引入的条件生成对抗网络(CGAN)的强大生成建模能力，**通过施加额外的约束**，即去雨图像必须与其对应的地面真实干净图像不可区分。来自GAN的对抗性损失提供了额外的规律性，并有助于实现更好的结果。除了提出一种去雨图像的新方法外，我们还**在生成器-鉴别器对中引入了一种新的精化损失函数和结构新颖性，以获得更好的结果**。Lost函数旨在减少Gans引入的伪影，并确保更好的视觉质量。生成器子网络是使用最近引入的密集连接网络来构建的，而鉴别器被设计为利用全局和局部信息来判断图像的真伪。</span>在此基础上，我们提出了一种新的单幅图像去雨方法--图像去雨条件生成对抗性网络(ID-CGAN)，该方法在目标函数中综合考虑了量化性能、视觉性能和判别性能。在合成图像和真实图像上的实验表明，该方法在量化和视觉性能方面都优于目前最先进的单幅图像去雨方法。此外，在FasterRCNN的目标检测数据集上的实验结果也证明了该方法在提高对降雨退化图像的检测性能方面的有效性。

[粗略讲解](https://blog.csdn.net/mmdbhs/article/details/122170935)









###### Spatio-temporal fusion for remote sensing data: an overview and new benchmark 遥感时空融合：综述和新基准

摘要时空融合(STF)的目的是融合(时间密集的)粗分辨率图像和(时间稀疏的)精细分辨率图像，以生成具有足够的时间和空间分辨率的图像序列。在过去的十年里，STF引起了人们的极大关注，并发展了许多STF方法。然而，到目前为止，STF领域仍然缺乏基准数据集，这是一个迫切需要解决的问题，以促进该领域的发展。<span style=color:blue>在这篇综述中，我们(在文献中第一次)提供了一个健壮的基准STF数据集，它包括三个重要特征：(1)区域多样性，(2)长时间跨度，(3)具有挑战性的场景。我们还提供了具有高度代表性的STF技术的调查，以及它们的性能与我们新提出的基准数据集的详细定量和定性比较。拟议的数据集是公开的，并可在网上获得</span>。时空融合；多时相遥感数据；基准数据；实验验证

[【文献阅读】](https://zhuanlan.zhihu.com/p/471885193)

https://blog.csdn.net/qq_43307074/article/details/127247752)
