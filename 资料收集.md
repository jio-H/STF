工具类

[CVPR 2023 论文和开源项目合集(Papers with Code)](https://github.com/amusi/CVPR2023-Papers-with-Code)

[记录每天整理的计算机视觉/深度学习/机器学习相关方向的论文](https://github.com/amusi/daily-paper-computer-vision)

[Awesome-Data-Fusion-for-Remote-Sensing](https://github.com/px39n/Awesome-Data-Fusion-for-Remote-Sensing)

[Hyperspectral-Image-Super-Resolution-Benchmark](https://github.com/junjun-jiang/Hyperspectral-Image-Super-Resolution-Benchmark)

[Awesome-Data-Fusion-for-Remote-Sensing](https://github.com/px39n/Awesome-Data-Fusion-for-Remote-Sensing)

[GPT 学术优化 (GPT Academic)](https://github.com/jio-H/gpt_academic)

[EDVR作者主页](https://www.mmlab-ntu.com/person/ccloy/)





学习类



:ballot_box_with_check: [（附文献和代码）多源遥感影像时空融合技术介绍](https://zhuanlan.zhihu.com/p/601897889)

[图像融合常用数据集整理](https://zhuanlan.zhihu.com/p/508051065)

[[遥感]遥感**变化检测**文献/数据集资源列表推荐](https://zhuanlan.zhihu.com/p/528959742)

[笔记 | 基于深度学习的空谱遥感图像融合综述](https://zhuanlan.zhihu.com/p/569415087?utm_id=0)

[遥感分辨率及图像知识大全](https://zhuanlan.zhihu.com/p/437700025)

[条件生成对抗网络——cGAN原理与代码](https://zhuanlan.zhihu.com/p/629503280?utm_id=0)

[AIGC之图像生成内容介绍](https://zhuanlan.zhihu.com/p/629914637)

[VAE-变分自编码器系列](https://cloud.tencent.com/developer/article/2328570)



[其实SAM是个好老师 | SAMFeat教你如何结合自己的模型和SAM设计一个更好的模型](https://mp.weixin.qq.com/s/VOS0m7bMJU9E468Lk6eRNg)

[Landsat 和 MODIS 数据介绍（针对CIA,LGC数据集）](https://zhuanlan.zhihu.com/p/415449467)





## 时空融合的目的

目前单一卫星传感器获取的影像数据无法同时兼顾高空间分辨率和高时 间分辨率。因此，国内外学者提出了许多遥感图像时空融合方法，来生成同时具 有高空间分辨率与高时间分辨率的遥感图像

其实可以看作是带更多约束条件的分辨率，其目的是为了获取连续的、高空间分辨率的影像，解决光学遥感影像在**时间分辨率和空间分辨率的矛盾**：现有的大多数卫星获取的影像在空间分辨率能做到较高，那么它的回访周期就很漫长。

- **通俗理解**

卫星带着相机对准地表拍摄，并绕着地球旋转，那如果卫星拍摄区域小，那拍出来的照片的就更加清晰（空间分辨率高），但是要拍完整个地球要花更多时间（时间分辨率低），如果卫星拍的面积更大，拍完整个地球一遍要花更少时间，但是照片更加模糊。



## 解决方法

那么如何才能获取高时空分辨率的影像呢？为什么要用时空融合技术来实现呢？

实际上，现有的办法主要有三种：

（1）多个搭载相同传感器的卫星组网，例如sentinel-2A和sentinel-2B双星，回访周期缩短一半；

（2）提高传感器性能和卫星的观测技术；

（3）将高时间分辨率影像和高空间分辨率影像进行融合来获取高时空分辨率影像。



## 基础原理

时空融合的基础原理如下图所示，其本质是结合两类影像的时空优势。

![img](image/资料收集/v2-60ca33f0bdcdfbd3fa76fb72ea5fc08d_720w.webp)

T1时相为我们能获取到两类影像的时相，T2时相是只有高时间分辨率影像的时相

上图只考虑了一对可用影像的情况，实际上很多算法会使用多对历史影像来提高融合的可靠性，如下图：

![img](image/资料收集/v2-8150de4ce4e2b14376d90c0015b6d602_720w.webp)

图片来自文献[https://doi.org/10.3390/rs10040527]



## STFDCNN

STFDCNN算法，该算法中引入了卷积神经网络来建模空间分辨率不一致的问题，来生成具有高时空的高分辨率图像







## GAN的提出和应用

GAN首先由Goodfellow等人提出[25]，然后，基于GAN的应用出现了爆炸式增长，例如图像生成[26]，超分辨率[27]，去噪[28]，修复[29]和图像到图像的翻译[30]，显示了其强大的建模能力。

[GAN详解](https://zhuanlan.zhihu.com/p/408766083)





## DCSTFN





## 略读论文

<span style='color:red'>红色字体</span>：表示疑问的地方

<span style="background-color: yellow;">黄色标记</span>：表示使用的方法

<span style=color:blue>蓝色字体</span>：表示解决的问题



###### Image De-raining Using a Conditional Generative Adversarial Network

摘要-恶劣的天气条件，如雨雪，对在这种条件下拍摄的图像的视觉质量产生不利影响，从而使它们无法进一步使用和共享。此外，这种退化的图像极大地影响了视觉系统的性能。因此，<span style=color:blue>解决单幅图像去雨问题</span>是非常重要的。然而，这个问题<span style='color:red'>固有的不适定性</span>带来了几个挑战。<span style="background-color: yellow;">我们试图利用最近引入的条件生成对抗网络(CGAN)的强大生成建模能力，**通过施加额外的约束**，即去雨图像必须与其对应的地面真实干净图像不可区分。来自GAN的对抗性损失提供了额外的规律性，并有助于实现更好的结果。除了提出一种去雨图像的新方法外，我们还**在生成器-鉴别器对中引入了一种新的精化损失函数和结构新颖性，以获得更好的结果**。Lost函数旨在减少Gans引入的伪影，并确保更好的视觉质量。生成器子网络是使用最近引入的密集连接网络来构建的，而鉴别器被设计为利用全局和局部信息来判断图像的真伪。</span>在此基础上，我们提出了一种新的单幅图像去雨方法--图像去雨条件生成对抗性网络(ID-CGAN)，该方法在目标函数中综合考虑了量化性能、视觉性能和判别性能。在合成图像和真实图像上的实验表明，该方法在量化和视觉性能方面都优于目前最先进的单幅图像去雨方法。此外，在FasterRCNN的目标检测数据集上的实验结果也证明了该方法在提高对降雨退化图像的检测性能方面的有效性。

[粗略讲解](https://blog.csdn.net/mmdbhs/article/details/122170935)











###### PAN-GAN

2020年

摘要

遥感图像融合中的泛锐化是指将全色图像和低分辨率的多光谱图像进行融合，得到高分辨率的多光谱图像。近年来，基于卷积神经网络(CNN)的泛锐化方法取得了最好的效果。<span style=color:blue>尽管如此，仍然存在两个问题。一方面，现有的基于CNN的策略需要监督，低分辨率的多光谱图像是通过简单地对高分辨率的图像进行模糊和下采样来获得的。另一方面，它们通常忽略了全色图像丰富的空间信息。</span>为了解决这些问题，我们提出了一种基于产生式对抗网络的无监督泛锐化框架，称为Pangan，它在网络训练过程中不依赖于所谓的地面事实。在我们的方法中，生成器分别与光谱鉴别器和空间鉴别器建立对抗性博弈，从而保留了多光谱图像丰富的光谱信息和全色图像的空间信息。通过大量的实验验证了所提出的泛GaN锐化方法与其他先进的泛化锐化方法相比的有效性。我们的PanGAN在定性视觉效果和定量评估指标方面表现出了良好的性能。







###### 对齐问题到底是什么？

特征对齐

[Re-ID中的特征对齐](https://zhuanlan.zhihu.com/p/126722559)







###### 知识蒸馏

[知识蒸馏方法总结（持续更新）](https://zhuanlan.zhihu.com/p/603748226)

[知识蒸馏（一）概述](https://zhuanlan.zhihu.com/p/581286422)

[知识蒸馏（Knowledge Distillation）简述（一）](https://zhuanlan.zhihu.com/p/81467832)

大概的理解

知识蒸馏，可以将一个网络的知识转移到另一个网络，两个网络可以是同构或者异构。做法是先训练一个teacher网络，然后使用这个teacher网络的输出和数据的真实标签去训练student网络。知识蒸馏，可以用来将网络从大网络转化成一个小网络，并保留接近于大网络的性能；也可以将多个网络的学到的知识转移到一个网络中，使得单个网络的性能接近emsemble的结果。







###### ViTGAN: Training GANs with Vision Transformers

中探索了使用 ViT 作为生成对抗网络 (GAN) 中图像合成任务的生成器和判别器的潜力。本文研究了在 GAN 背景下 ViT 与传统卷积架构的性能比较。

该论文的主要发现包括：

**基于 ViT 的 GAN 的性能与卷积 GAN 相当：**结果表明，利用 ViT 作为生成器和判别器的 ViTGAN 在图像合成质量方面与传统卷积 GAN（例如 StyleGAN2）具有相似的性能。

**改进的架构灵活性：**与卷积 GAN 相比，ViTGAN 提供了增强的架构灵活性。作者证明，ViTGAN 可以通过调整 Transformer 层的数量或 token 的数量来扩展，而不改变空间维度，从而更容易使模型适应不同的计算预算和问题规模。

**有效的迁移学习：**研究表明，预训练的 ViT 模型可以成功地作为 GAN 中的生成器进行微调，从而显著减少训练时间，同时保持竞争性能。这一发现强调了在图像合成任务中使用预先训练的 ViT 进行迁移学习的潜力。

**对输入变化的鲁棒性：**ViTGAN 对输入分布的变化表现出鲁棒性，如对损坏和分布外样本的实验所示。作者认为，ViT 中的注意力机制有助于这种鲁棒性，使模型能够有效地处理输入数据的变化。

总之，这项研究强调了在生成对抗网络中使用 ViT 来执行图像合成任务的优势。研究结果表明，基于 ViT 的 GAN 在图像质量、架构灵活性和迁移学习潜力方面优于传统的卷积 GAN。这些见解可以指导研究人员和从业者为图像合成应用选择合适的架构，并进一步探索 ViT 在该领域的潜力。
