

## @unique

```python
@unique
class Mode(Enum):
    '''
    Defines the different mode of the application.
    '''
    TRAINING = auto()
    VALIDATION = auto()
    PREDICTION = auto()
```

在Python中，@unique是一个装饰器（decorator），用于标记一个枚举类中的成员是唯一的。当使用@unique装饰器时，它会确保枚举类中的成员值是唯一的，如果有重复的成员值，则会抛出ValueError异常。这可以帮助开发者避免在枚举类中定义重复的成员值。以下是一个示例：

```python
from enum import Enum, unique

@unique
class Color(Enum):
    RED = 1
    GREEN = 2
    BLUE = 3
    RED = 4

# 由于有重复的成员值，会抛出异常
# ValueError: duplicate values found in <enum 'Color'>: RED -> RED
```

在上面的例子中，由于`RED`成员值重复定义了两次，所以会抛出ValueError异常。如果将`@unique`装饰器移除，程序将可以正常运行，但是枚举类中的成员值将不再是唯一的。



## @torch.no_grad()

`@torch.no_grad()`是一个装饰器，用于在代码块中禁用梯度计算。

当使用`@torch.no_grad()`装饰一个函数或代码块时，其中的所有张量操作都不会被记录梯度。这对于在推断阶段或者不需要计算梯度的代码段非常有用，因为禁用梯度计算可以提高代码的执行效率。

在PyTorch中，默认情况下，所有的张量操作都会被自动跟踪并记录梯度，以便进行反向传播和梯度更新。但是，在某些情况下，我们可能不希望计算梯度，例如在评估模型性能或进行预测时。使用`@torch.no_grad()`可以临时禁用梯度计算，以提高代码的执行效率。

以下是一个使用`@torch.no_grad()`装饰器的示例：

```
@torch.no_grad()
def evaluate(model, data_loader):
    # 禁用梯度计算
    with torch.no_grad():
        for inputs, labels in data_loader:
            # 前向传播
            outputs = model(inputs)
            # 计算损失或评估指标
            loss = loss_fn(outputs, labels)
            # ...
```

在上述示例中，`evaluate`函数使用`@torch.no_grad()`装饰器来禁用梯度计算。在函数内部，通过`with torch.no_grad():`语句块进一步确保在其中的代码不会计算梯度。这样可以避免不必要的计算和内存消耗，提高代码的执行效率。



## TRAINING = auto()

在这个例子中，`auto()`函数被用于自动为`TRAINING`成员分配一个唯一的值。`auto()`函数是`enum`模块中的一个函数，它会根据之前定义的成员值来自动分配下一个唯一的整数值。

在这个例子中，`TRAINING`成员被赋予了一个自动生成的唯一值。具体的值取决于之前定义的成员值。如果之前已经定义了其他成员，那么`auto()`函数会根据最后一个成员的值来自动分配下一个唯一的整数值。

以下是一个示例：

````python
from enum import Enum, auto

class Mode(Enum):
    在这个例子中，`auto()`函数被用于自动为`TRAINING`成员分配一个唯一的值。`auto()`函数是`enum`模块中的一个函数，它会根据之前定义的成员值来自动分配下一个唯一的整数值。

在这个例子中，`TRAINING`成员被赋予了一个自动生成的唯一值。具体的值取决于之前定义的成员值。如果之前已经定义了其他成员，那么`auto()`函数会根据最后一个成员的值来自动分配下一个唯一的整数值。

以下是一个示例：

```python
from enum import Enum, auto

class Mode(Enum):
    TRAINING = auto()
    VALIDATION = auto()
    PREDICTION = auto()

print(Mode.TRAINING.value)  # 输出 1
print(Mode.VALIDATION.value)  # 输出 2
print(Mode.PREDICTION.value)  # 输出 3
```

在这个例子中，`TRAINING`成员被自动分配了值1，`VALIDATION`成员被自动分配了值2，`PREDICTION`成员被自动分配了值3。这样，可以确保每个成员值都是唯一的，并且不需要手动指定值。
    VALIDATION = auto()
    PREDICTION = auto()

print(Mode.TRAINING.value)  # 输出 1
print(Mode.VALIDATION.value)  # 输出 2
print(Mode.PREDICTION.value)  # 输出 3
````

在这个例子中，`TRAINING`成员被自动分配了值1，`VALIDATION`成员被自动分配了值2，`PREDICTION`成员被自动分配了值3。这样，可以确保每个成员值都是唯一的，并且不需要手动指定值。

## @staticmethod

在Python中，`@staticmethod`是一个装饰器（decorator），用于定义一个静态方法（static method）。静态方法是属于类而不是实例的方法，可以通过类名直接调用，而无需创建类的实例。

使用`@staticmethod`装饰器可以将一个类中的方法标记为静态方法。静态方法不需要访问实例属性或方法，因此不需要传递`self`参数。静态方法通常用于执行与类相关的操作，但不需要访问实例的状态。

以下是一个示例：

```python
class MathUtils:
    @staticmethod
    def add(x, y):
        return x + y

result = MathUtils.add(3, 4)
print(result)  # 输出 7
```

在这个例子中，`MathUtils`类定义了一个静态方法`add`。通过`@staticmethod`装饰器，将`add`方法标记为静态方法。然后，可以直接通过类名`MathUtils`调用静态方法`add`，而无需创建类的实例。

静态方法在访问类属性或调用其他静态方法时，应该使用类名作为限定符。它们与实例方法和类方法不同，不会自动传递实例或类作为第一个参数。因此，在静态方法内部无法直接访问实例属性或调用实例方法。

## torch.manual_seed(2020)

```python
if opt.cuda:
    torch.cuda.manual_seed_all(2020)
    cudnn.benchmark = True
    cudnn.deterministic = True
```



以上代码片段是在使用PyTorch进行深度学习任务时常见的设置，用于配置CUDA和cuDNN的随机数种子和性能优化选项。

`if opt.cuda:` 是一个条件语句，判断是否启用CUDA加速。如果`opt.cuda`为真（即启用CUDA），则执行以下代码块；否则，跳过该代码块。

`torch.cuda.manual_seed_all(2020)` 是设置所有可用的CUDA设备的随机数种子为2020。这将确保在使用CUDA进行计算时，生成的随机数序列是可重复的。

`cudnn.benchmark = True` 是启用cuDNN的自动优化选项。cuDNN是一个针对深度学习任务的GPU加速库，可以显著提高计算性能。启用自动优化将根据输入数据的大小和类型自动选择最佳的卷积算法，以提高计算速度。但是，如果输入数据的大小和类型不变，建议将其设置为False，以获得更稳定的结果。

`cudnn.deterministic = True` 是启用cuDNN的确定性选项。启用确定性选项将禁用一些非确定性的算法，以确保在相同的输入数据和参数下，计算结果是确定的。这通常用于确保在训练和推理过程中产生相同的结果，以便进行结果的可重复性验证。

这些设置通常在深度学习任务中使用，以确保结果的可重复性，并获得最佳的性能和稳定性。



## nn.DataParallel

```python
if isinstance(model, nn.DataParallel):
        model = model.module
```

这段代码用于检查`model`是否是`nn.DataParallel`的实例，如果是，则将`model`转换为其内部的`module`。

`nn.DataParallel`是PyTorch中的一个模型包装器，用于在多个GPU上并行地运行模型。当使用`nn.DataParallel`封装模型时，`model`实际上是一个`nn.DataParallel`对象，而不是模型本身。

在这段代码中，`isinstance(model, nn.DataParallel)`用于检查`model`是否是`nn.DataParallel`的实例。如果是，那么`model`是一个被`nn.DataParallel`封装的模型。

然后，通过`model.module`可以获取到`nn.DataParallel`内部的模型。`model.module`返回的是被封装在`nn.DataParallel`中的原始模型对象，而不是`nn.DataParallel`对象本身。

通过将`model`重新赋值为`model.module`，可以将`model`转换为原始模型对象，以便后续的操作和使用。这在某些情况下可能是必要的，例如在保存或加载模型时，或者在使用其他模块或函数处理模型时。





## 继续训练

```python
# 如果resume为True，且训练历史记录存在，则加载上一次训练的模型
if resume and self.history.exists():
    df = pd.read_csv(self.history)
    last_epoch = int(df.iloc[-1]['epoch'])
    least_error = df['val_error'].min()
    load_checkpoint(self.last_g, self.generator, optimizer=self.g_optimizer)
    load_checkpoint(self.last_d, self.discriminator, optimizer=self.d_optimizer)
    start_epoch = last_epoch + 1

```





## rasterio.open

`rasterio.open` 是 rasterio 库中的一个函数，用于打开和读取栅格数据文件。

它的一般用法是：
```python
with rasterio.open(file_path) as src:
    # 对打开的数据进行操作
```

其中 `file_path` 是要打开的**栅格数据文件**的路径。使用 `with` 语句可以确保在使用完数据后，正确地关闭数据文件。

在 `with` 代码块中，你可以对打开的数据进行各种操作，例如读取数据、获取元数据、执行空间转换等。

请注意，要使用 `rasterio.open` 函数，你需要先安装并导入 `rasterio` 库。

### 栅格数据文件

栅格数据文件是一种用于存储和处理空间数据的文件格式。它将空间数据划分为规则的栅格单元，并在每个栅格单元中存储一个值或一组值。

栅格数据文件通常用于存储遥感图像、地理信息系统（GIS）数据、数字高程模型（DEM）、气候数据等。常见的栅格数据文件格式包括 GeoTIFF、NetCDF、HDF5 等。

这些文件格式可以存储多种类型的栅格数据，例如灰度图像、彩色图像、地表高度、温度、降水量等。每个栅格单元的值可以表示某个属性的测量值、分类标签、像素亮度等。

通过使用栅格数据文件，可以对空间数据进行分析、可视化和处理，以提取有关地理现象的信息，并支持各种应用领域，如遥感、地理信息系统、地质学、环境科学等。





## pathlib.Path

[深入理解posixpath](https://www.python100.com/html/I7NTWG53165J.html)

`pathlib.Path` 是 Python 中用于处理文件路径的标准库。它提供了一种面向对象的方式来操作文件和目录路径。

使用 `pathlib.Path`，你可以轻松地创建、操作和访问文件和目录的路径。它提供了一系列方法和属性，例如 `joinpath()`、`resolve()`、`is_file()`、`is_dir()` 等，用于构建路径、解析路径、检查路径是否是文件或目录等操作。

以下是一个简单的示例代码，展示了如何使用 `pathlib.Path`：

```python
import pathlib

# 创建一个 Path 对象
path = pathlib.Path('/path/to/file.txt')

# 获取文件名
print(path.name)  # 输出 'file.txt'

# 获取文件所在目录
print(path.parent)  # 输出 '/path/to'

# 检查路径是否存在
print(path.exists())  # 输出 True

# 检查路径是否是文件
print(path.is_file())  # 输出 True

# 检查路径是否是目录
print(path.is_dir())  # 输出 False

# 解析路径
resolved_path = path.resolve()
print(resolved_path)  # 输出实际的绝对路径

# 连接路径
new_path = path.joinpath('new_file.txt')
print(new_path)  # 输出 '/path/to/new_file.txt'
```

通过使用 `pathlib.Path`，你可以更加简洁和可读地处理文件和目录路径，而无需手动拼接字符串或使用其他复杂的方法。



## Python 打印树形目录结构 （类似 linux 下目录的形式）

```
import os
import os.path


def dfs_showdir(path, depth):
    if depth == 0:
        print("root:[" + path + "]")

    for item in os.listdir(path):
        if '.git' not in item:
            print("|      " * depth + "|--" + item)
            newitem = path + '/' + item
            if os.path.isdir(newitem):
                dfs_showdir(newitem, depth + 1)


if __name__ == '__main__':
    dfs_showdir('D:\MyLearning\PyCharm\myQt01', 0) # 这里输入需要导出目录结构的根路径**加粗样式**


```



## 将`__getitem__`方法返回的元素组织为列表或其他

list或字典中的每个元素都是一个数据样本

```python
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
import torch

# 假设 dataset 是包含 __getitem__ 方法的数据集对象
# 这个数据集返回的样本是 1x3 的列表，每个元素都是 (3, 32, 32) 的张量

# 创建数据加载器
class MyDataset(Dataset):
    def __init__(self):
        super(MyDataset, self).__init__

    def __getitem__(self, index):
        return [torch.rand(3, 32, 32)]*3
    def __len__(self):
        return 5

dataset = MyDataset()
dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

# 每个批次中包含 4 个样本
for batch in dataloader:
    for i in batch:
        print(i.shape)
        
# torch.Size([4, 3, 32, 32])
# torch.Size([4, 3, 32, 32])
# torch.Size([1, 3, 32, 32])
# torch.Size([1, 3, 32, 32])
# torch.Size([1, 3, 32, 32])
```



通常情况下，将`__getitem__`方法返回的元素组织为列表，其中每个元素表示一个数据样本，是因为这种方式在处理各种数据集时非常灵活和通用。以下是一些原因：

1. **多样本处理：** 数据集通常包含多个数据样本，将每个样本表示为列表中的元素使得处理多个样本更加方便。您可以轻松地通过迭代列表来处理每个样本，而不必担心样本的数量。

2. **数据类型多样性：** 数据集中的数据样本可能具有不同的类型，例如图像、文本、数字等。将它们组织为列表允许您以一致的方式处理各种数据类型。

3. **批次处理：** 在训练深度学习模型时，通常使用批次进行前向传播和反向传播。将数据样本组织为列表，然后在数据加载器中选择批次大小，可以轻松实现批次处理。

4. **数据增强：** 数据增强技术（如随机裁剪、翻转等）通常会应用于单个数据样本。将每个数据样本作为列表元素允许您对每个样本独立应用数据增强。

5. **一致性：** 以列表的形式表示数据样本在处理和操作时更具一致性。这使得数据集、数据加载器和模型之间的接口更容易管理和理解。

总之，将`__getitem__`方法返回的数据样本组织为列表是为了提供更大的灵活性和通用性，适应不同类型的数据和处理需求。这使得处理数据集更加方便，并为深度学习任务提供了一种一致的方法。然而，**根据特定的应用和数据集，也可以选择其他数据组织方式**，例如返回字典形式的数据样本。